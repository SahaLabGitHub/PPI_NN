{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Install Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4gtCmZ6xirc",
    "outputId": "a627c57e-0a32-4746-8b27-343480173997"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "# Install external libraries\n",
    "%pip install umap-learn\n",
    "%pip install shap\n",
    "%pip install seaborn\n",
    "%pip install plotly\n",
    "%pip install tensorflow\n",
    "\n",
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import umap.umap_ as umap\n",
    "import random as python_random\n",
    "from scipy.stats import normaltest, pearsonr\n",
    "\n",
    "# Google library\n",
    "#from google.colab import drive\n",
    "\n",
    "# Graphic libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "# Scikit-learn libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Tensorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shap\n",
    "\n",
    "# Set seeds\n",
    "umap_limit=1\n",
    "seed_value=13\n",
    "tf.random.set_seed(seed_value)\n",
    "python_random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# Check if gpu is enabled\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(file_path, remove_anomalous=False):\n",
    "    \"\"\"Load and preprocess training data from a given file path\"\"\"\n",
    "    df_energy = pd.read_csv(file_path)\n",
    "    df_energy = df_energy.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Remove anomalous PDB entries if specified\n",
    "    if remove_anomalous:\n",
    "        anomaly_pdb_id = pd.read_csv(\"/Users/qingshuzhao/Documents/machine learning PPI project/anomaly_mapping.csv\")\n",
    "        df_energy = df_energy[~df_energy['pdb_id'].isin(anomaly_pdb_id['anomaly_id'])]\n",
    "    \n",
    "    # Add experimental binding energy\n",
    "    df_energy['dG_exp'] = (1.98722*298.15*np.log(df_energy['kd_molar'])/1000)\n",
    "    df_energy.drop(columns=['resolution', 'kd_molar'], axis=1, inplace=True)\n",
    "    pdb_id = df_energy.pop('pdb_id')\n",
    "    \n",
    "    return df_energy\n",
    "\n",
    "\n",
    "def get_predictions(X, models):\n",
    "    \"\"\"Get predictions from ensemble of models\"\"\"\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict(X, verbose=0)\n",
    "        predictions.append(pred)\n",
    "    predictions = np.array(predictions)\n",
    "    mean_pred = np.mean(predictions, axis=0)\n",
    "    std_pred = np.std(predictions, axis=0)\n",
    "    return mean_pred.flatten(), std_pred.flatten()\n",
    "\n",
    "\n",
    "def train_model(df_energy):\n",
    "    \"\"\"Train multiple neural network models on given data\"\"\"\n",
    "    global X_train  # Make X_train global so it can be accessed by plot_shap_importance\n",
    "    \n",
    "    X = df_energy.drop(columns=['dG_exp'], axis=1)\n",
    "    y = df_energy[\"dG_exp\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                        random_state=seed_value,\n",
    "                                                        shuffle=True)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train),\n",
    "                          index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test),\n",
    "                         index=X_test.index, columns=X_test.columns)\n",
    "    \n",
    "    # UMAP reduction and model training\n",
    "    umap_dict = {}\n",
    "    models_dict = {}\n",
    "    \n",
    "    for i in range(umap_limit):\n",
    "        reducer = umap.UMAP(n_components=15, n_neighbors=20, random_state=42, transform_seed=42, verbose=False)\n",
    "\n",
    "        X_t = reducer.fit_transform(X_train)\n",
    "        X_t = pd.DataFrame(data=X_t, columns=[f\"column_{i}\" for i in range(X_t.shape[1])])\n",
    "\n",
    "        X_te = reducer.transform(X_test)\n",
    "        X_te = pd.DataFrame(data=X_te, columns=[f\"column_{i}\" for i in range(X_te.shape[1])])\n",
    "\n",
    "        umap_dict[str(i)] = (X_t, X_te, reducer)\n",
    "\n",
    "        # Train 10 models for ensemble\n",
    "        models = []\n",
    "        for j in range(2):\n",
    "            model = keras.Sequential([\n",
    "                layers.Dense(64, activation='relu', input_shape=(15,)),\n",
    "                layers.Dense(32, activation='relu'),\n",
    "                layers.Dense(16, activation='relu'),\n",
    "                layers.Dense(1)\n",
    "            ])\n",
    "            \n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "            \n",
    "            # Use K-fold validation\n",
    "            kfold = KFold(n_splits=10, shuffle=True, random_state=seed_value)\n",
    "            for train, val in kfold.split(X_t, y_train):\n",
    "                history = model.fit(\n",
    "                    X_t.iloc[train], y_train.iloc[train],\n",
    "                    validation_data=(X_t.iloc[val], y_train.iloc[val]),\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    verbose=0,\n",
    "                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lambda epoch, lr: float(lr * tf.math.exp(-0.1)) if epoch in [5, 50, 150] else lr)]\n",
    "                )\n",
    "            models.append(model)\n",
    "            print(f\"Model {i}-{j} trained\")\n",
    "            \n",
    "        models_dict[str(i)] = models\n",
    "\n",
    "    return models_dict, umap_dict, scaler\n",
    "\n",
    "\n",
    "def evaluate_model(models_dict, umap_dict, scaler, test_data, dataset_name, training_file):\n",
    "    \"\"\"Evaluate ensemble model on test dataset\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    global training_name, results_dir  # Make results_dir global\n",
    "    # Create timestamped directory for this training run\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M\")\n",
    "    training_name = os.path.splitext(os.path.basename(training_file))[0]\n",
    "    results_dir = f\"/Users/qingshuzhao/Documents/machine learning PPI project/training_results/{training_name}_{timestamp}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(umap_limit):\n",
    "        preds = []\n",
    "\n",
    "        columns_to_drop = ['experimental', 'packstat', 'SCORE:', 'yhh_planarity', 'description']\n",
    "        X_v = test_data.drop(columns=[col for col in columns_to_drop if col in test_data.columns], axis=1).copy()\n",
    "        y_v = test_data[\"experimental\"].copy()\n",
    "        \n",
    "        X_v = pd.DataFrame(scaler.transform(X_v),\n",
    "                        index=X_v.index, columns=X_v.columns)\n",
    "\n",
    "        u = umap_dict[str(i)][2]\n",
    "        X_v = u.transform(X_v)\n",
    "        X_v = pd.DataFrame(data=X_v, columns=[f\"column_{c}\" for c in range(X_v.shape[1])])\n",
    "\n",
    "        test_predictions, std = get_predictions(X_v, models_dict[str(i)])\n",
    "        test = y_v.to_numpy()\n",
    "\n",
    "        metrics['rmse'] = np.sqrt(mean_squared_error(test, test_predictions, squared=True))\n",
    "        metrics['r2_ann'] = r2_score(test, test_predictions)\n",
    "        metrics['pearson_ann'] = pearsonr(test, test_predictions)[0]\n",
    "\n",
    "        print(f\"\\nMetrics for {dataset_name}:\")\n",
    "        print(f\"RMSE score: {metrics['rmse']:.4f}\")\n",
    "        print(f\"R2 score experimental vs ANN: {metrics['r2_ann']:.4f}\")\n",
    "        print(f\"Pearson correlation experimental vs ANN: {metrics['pearson_ann']:.4f}\")\n",
    "        print(f\"Average prediction uncertainty: {np.mean(std):.4f}\")\n",
    "     \n",
    "        preds.append((test_predictions, test))\n",
    "\n",
    "        df_comparison = pd.DataFrame({\n",
    "            \"REAL\": test,\n",
    "            \"ANN_PREDICTED\": test_predictions,\n",
    "            \"ANN_ERROR\": test - test_predictions,\n",
    "            \"PREDICTION_STD\": std,\n",
    "            \"PDB_ID\": test_data['description'].values\n",
    "        })\n",
    "\n",
    "        output_path = os.path.join(results_dir, f\"{dataset_name}_comparison.csv\")\n",
    "        df_comparison.to_csv(output_path, index=True)\n",
    "        print(f\"\\nResults saved to: {output_path}\")\n",
    "\n",
    "    return metrics, results_dir\n",
    "\n",
    "\n",
    "def plot_shap_importance(model, umap_dict, models_dict, n_models=1):\n",
    "    # Get UMAP components\n",
    "    components = umap_dict[\"0\"][1].columns\n",
    "    \n",
    "    # Calculate SHAP values for each model\n",
    "    shap_dict = {}\n",
    "    shap_values_list = []\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        explainer = shap.DeepExplainer(models_dict[str(i)][0], umap_dict[str(i)][0].to_numpy())\n",
    "        shap_values = explainer.shap_values(umap_dict[str(i)][1].to_numpy())\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = np.sum(np.array(shap_values), axis=0)\n",
    "        shap_values = np.squeeze(shap_values)\n",
    "        shap_values_list.append(shap_values)\n",
    "        \n",
    "        print(f\"Shape of values[{i}]: {shap_values_list[i].shape}\")\n",
    "        \n",
    "        rf_resultX = pd.DataFrame(shap_values_list[i], columns=components)\n",
    "        vals = np.abs(rf_resultX.values).mean(0)\n",
    "        \n",
    "        shap_importance = pd.DataFrame(list(zip(components, vals)),\n",
    "                                     columns=['col_name','feature_importance_vals'])\n",
    "        shap_importance.sort_values(by=['feature_importance_vals'],\n",
    "                                  ascending=False, inplace=True)\n",
    "        \n",
    "        shap_dict[i] = shap_importance\n",
    "\n",
    "    # Average SHAP values across models\n",
    "    shap_df = shap_dict[0]\n",
    "    for i in range(1, n_models):\n",
    "        shap_df['feature_importance_vals'] += shap_dict[i][\"feature_importance_vals\"]\n",
    "    shap_df['feature_importance_vals'] /= n_models\n",
    "\n",
    "    # Get original features and correlations\n",
    "    features = X_train.columns\n",
    "    corr = pd.concat([X_train, umap_dict[\"0\"][0]], axis=1).corr()\n",
    "\n",
    "    # Calculate final feature importance values\n",
    "    final_values = {}\n",
    "    for f in features:\n",
    "        s = .0\n",
    "        for c in components:\n",
    "            score = shap_df[shap_df[\"col_name\"] == c]\n",
    "            s += score[\"feature_importance_vals\"].iloc[0] * abs(corr[f][c])\n",
    "        final_values[f] = s\n",
    "\n",
    "    # Sort values\n",
    "    sorted_dict = {}\n",
    "    sorted_keys = sorted(final_values, key=final_values.get, reverse=True)\n",
    "    for w in sorted_keys:\n",
    "        sorted_dict[w] = final_values[w]\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.barh(list(sorted_dict.keys()), sorted_dict.values(), color='b')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return pd.DataFrame(list(sorted_dict.items()), columns=['Feature', 'Importance'])\n",
    "\n",
    "\n",
    "def main():\n",
    "    training_dir = \"/Users/qingshuzhao/Documents/machine learning PPI project/training_dataset/new\"\n",
    "    \n",
    "    nanobody_data = pd.read_csv(\"/Users/qingshuzhao/Documents/machine learning PPI project/testing_dataset/vhhs2_experimental.csv\")\n",
    "    pdbind_data = pd.read_csv(\"/Users/qingshuzhao/Documents/machine learning PPI project/testing_dataset/validation.csv\")\n",
    "    nazmul_data = pd.read_csv(\"/Users/qingshuzhao/Documents/machine learning PPI project/testing_dataset/Nazmul_11_scores_experimental.csv\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    training_files = []\n",
    "    for file_name in os.listdir(training_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(training_dir, file_name)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            training_files.append((file_name, file_path, file_size))  # Store both name and path\n",
    "    \n",
    "    training_files.sort(key=lambda x: x[2])  # Sort by file size\n",
    "    \n",
    "    for file_name, file_path, _ in training_files:\n",
    "        print(f\"\\nTraining with dataset: {file_name}\")\n",
    "        \n",
    "        df_energy = load_training_data(file_path)\n",
    "        models_dict, umap_dict, scaler = train_model(df_energy)\n",
    "        \n",
    "        # Generate SHAP importance plot for this model\n",
    "        importance_df = plot_shap_importance(None, umap_dict, models_dict)\n",
    "        \n",
    "        test_datasets = {\n",
    "            'Nanobody': nanobody_data,\n",
    "            'PDBind': pdbind_data,\n",
    "            'Nazmul': nazmul_data\n",
    "        }\n",
    "        \n",
    "        for dataset_name, test_data in test_datasets.items():\n",
    "            metrics, results_dir = evaluate_model(models_dict, umap_dict, scaler, test_data, dataset_name, file_path)\n",
    "            \n",
    "            # Save feature importance results after evaluate_model creates results_dir\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M\")\n",
    "            importance_path = os.path.join(results_dir, f\"feature_importance_{os.path.splitext(file_name)[0]}.csv\")\n",
    "            importance_df.to_csv(importance_path)\n",
    "            print(f\"Feature importance saved to: {importance_path}\")\n",
    "            \n",
    "            results.append({\n",
    "                'training_file': file_name,\n",
    "                'test_dataset': dataset_name,\n",
    "                'metrics': metrics})\n",
    "\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f\"/Users/qingshuzhao/Documents/machine learning PPI project/training_results/metrics_{timestamp}.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_model+importance_plot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
